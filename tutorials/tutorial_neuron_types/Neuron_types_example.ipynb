{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "<img src=\"flow.png\" width=\"700\">\n",
    "\n",
    "In this tutorial we show how PartSeg API can be used to build composite processing pipelines in Python (see the overview of the pipeline in the figure). We show how to develop a browser base open-source Jupyter notebook for performing of segmentation of nuclei from 3-D images using PartSeg components as libraries. We describe how to automatically filter out properly segmented nuclei and later how to divide them based on presence/ absence of specific staining, into classes, which can be analysed separately.\n",
    "\n",
    "\n",
    "## Motivation\n",
    "Often biologist work on mixed population of cells, where subpopulations can be detected based on specific markers, like presence or absence of particular tags or proteins.\n",
    "\n",
    "For quantitative measurements we want to process as many cases as possible. Yet, processing of hundreds of stacks and grouping them manually is laborious and otherwise could require hours of manual sorting of imaging data. Therefore automatic recognition of specific types of cells is necessary. This motivates collaboration between experimental biologists and bioinformaticians. Here we present how PartSeg components can be used as part of a larger application implemented in as Jupyter notebook. The pipeline allows to automatize quantitative analysis of different neuronal subtypes.\n",
    "\n",
    "We argue that exposing the functionality of PartSeg in Python rather than designing a plugin or scripting module for PartSeg is preferable for bioinformaticians as Python is a general and well-known (low entry-cost) programming language with wealth of scientific libraries. At the same time keeping the UI simple and uncluttered is preferable for experimental biologists (again low entry-cost).\n",
    "\n",
    "\n",
    "## Content\n",
    "\n",
    "For the purpose of this tutorial in vitro culture of hippocampal neurons was fixed and subjected to fluorescent immunostaining with antibodies specific for two neuronal markers- Prox1 and CamKII. Later 3D images were acquired using Zeiss 780 confocal microscope, with voxel size 77x77x210 nm in xyz plains.\n",
    "\n",
    "First channel of all images represent Prox1 staining, visible inside the nucleus and specific for subpopulation of excitatory neurons- granular neurons. Second channel shows CamKII enzyme present in the cell body (outside of the nucleus), which is characteristic for all excitatory neurons. Third and fourth channel represent DNA stained with Hoechst dye. The fourth channel is overexposed to facilitate segmentation of big nuclei containing less condensed, hence less visible DNA.\n",
    "\n",
    "Data contains 4 types of cells :\n",
    "1. Prox+, CamKII+ granular neurons (subtype of excitatory neurons).\n",
    "2. Prox-, CamKII+ pyramidal neurons (subtype of excitatory neurons).\n",
    "3. Prox-, CamKII- inhibitory neurons\n",
    "4. Prox+, CamKII- not well characterized, immature neurons observed in in vitro culture.\n",
    "\n",
    "\n",
    "## Remarks\n",
    "1. We suggest to start from the tutorial showing basic functionalities of Partseg available [here](http://nucleus3d.cent.uw.edu.pl/PartSeg/tutorials/tutorial_chromosome_1/). In some parts few alternative options are shown.\n",
    "2. Numbering of channels starts with 0.\n",
    "3. Training data can be downloaded from [here](http://nucleus3d.cent.uw.edu.pl/PartSeg/Downloads/neuron_types.zip), Jupyter notebook is available [here](http://nucleus3d.cent.uw.edu.pl/PartSeg/tutorials/tutorial_diferrent_neurons/Neuron_types_example.ipynb)\n",
    "4. Resulted code can be implemented into PartSeg body.\n",
    "5. Every json file can be exported from PartSeg.\n",
    "\n",
    "Data collection was carried out with the use of CePT infrastructure financed by the European Union - The European Regional Development Fund within the Operational Programme “Innovative economy” for 2007-2013.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np \n",
    "from collections import defaultdict\n",
    "import SimpleITK as sitk\n",
    "from glob import glob \n",
    "from PartSeg.tiff_image.image_reader import ImageReader\n",
    "from PartSeg.tiff_image.image import Image\n",
    "from math import pi\n",
    "from copy import deepcopy\n",
    "\n",
    "from PartSeg.segmentation_mask.stack_settings import StackSettings\n",
    "from PartSeg.utils.segmentation.segmentation_algorithm import ThresholdAlgorithm\n",
    "from PartSeg.utils.image_operations import dilate\n",
    "from PartSeg.utils.mask.io_functions import load_stack_segmentation, save_components\n",
    "from PartSeg.utils.analysis.statistics_calculation import Diameter, Volume\n",
    "from PartSeg.utils.mask.algorithm_description import mask_algorithm_dict\n",
    "from PartSeg.utils.convex_fill import convex_fill\n",
    "\n",
    "from PartSeg.segmentation_analysis.partseg_settings import PartSettings\n",
    "from PartSeg.utils.analysis.algorithm_description import analysis_algorithm_dict\n",
    "from PartSeg.utils.universal_const import Units\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "import PartSeg.plugins \n",
    "PartSeg.plugins.register()  # Load PartSeg plugins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/home/czaki/Obrazy/PartSeg/typy_neuronow/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data\n",
    "In this part there is shown how to read data and create or load segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters of segmentaion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters  of segmentation can be read from file  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_path, \"segment_data.json\"), 'r') as ff:\n",
    "    segmentation_description = json.load(ff, object_hook=StackSettings.decode_hook)\n",
    "    parameters = segmentation_description[\"values\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or setted manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PartSeg.utils.segmentation.noise_filtering import GaussType\n",
    "parameters = {'channel': 3,\n",
    " 'threshold': {'name': 'Manual', 'values': {'threshold': 19000}},\n",
    " 'minimum_size': 8000,\n",
    " 'close_holes': True,\n",
    " 'close_holes_size': 200,\n",
    " 'smooth_border': True,\n",
    " 'smooth_border_radius': 0,\n",
    " 'noise_removal': {'name': 'Gauss',\n",
    "  'values': {'gauss_type': GaussType.Layer, 'radius': 1.0}},\n",
    " 'side_connection': False,\n",
    " 'use_convex': True}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = ImageReader.read_image(os.path.join(data_path, \"DMSO_120min_2_4.lsm\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Segentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment = ThresholdAlgorithm()\n",
    "segment.set_image(image)\n",
    "segment.set_parameters(**parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or \n",
    "Algorithm = mask_algorithm_dict[segmentation_description[\"algorithm\"]]\n",
    "segment = Algorithm()\n",
    "segment.set_image(image)\n",
    "segment.set_parameters(**parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = segment.calculation_run(print)\n",
    "segmentation = result.segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load segmentaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation, metadata = load_stack_segmentation(os.path.join(data_path, \"DMSO_120min_2_4.seg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Veriffy segmentaion \n",
    "In example stack DMSO_120min_2_4.lsm on standard parameters, there are two cases that should be filtered. \n",
    "1. Three nucleus that are too cloase and are segmentaed as one component (number 1)\n",
    "2. Nucles that touch border of image (numbers 10, 11, 12)\n",
    "\n",
    "When calculating sphericity (proportion beetwen volume and volume of sphere with same diameter like component.\n",
    "Threshold used in this step shoud depend on data. There are cell types with realy irregular shapes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_neurons = []\n",
    "for component_number in range(1, segmentation.max() + 1):\n",
    "    current_component_area = segmentation == component_number\n",
    "    # checking if touch borders \n",
    "    coords = np.nonzero(current_component_area)\n",
    "    if 0 == np.min(coords):\n",
    "        continue\n",
    "    touch = False\n",
    "    for axis_cords, max_size in zip(coords, current_component_area.shape):\n",
    "        if np.max(axis_cords) == max_size - 1:\n",
    "            touch = True\n",
    "            break\n",
    "    if touch:\n",
    "        continue\n",
    "    diameter = Diameter.calculate_property(current_component_area, voxel_size=image.spacing, result_scalar=10**6)\n",
    "    volume = Volume.calculate_property(current_component_area, voxel_size=image.spacing, result_scalar=10**6)\n",
    "    # calculate shericity\n",
    "    # print(component_number, (4/3 * pi * (diameter/2)**3)/volume, diameter)\n",
    "    if (4/3 * pi * (diameter/2)**3)/volume > 4:\n",
    "        continue \n",
    "    good_neurons.append(component_number)\n",
    "print(good_neurons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify neurons\n",
    "There are three types of neurons:\n",
    "1. granular - with red markers insideand green marker near surface\n",
    "2. pyramidal - with green marker near surface, but without red marker\n",
    "3. inhibitory - without both markers\n",
    "\n",
    "**In code channels are numbered from 0**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_type_dict = defaultdict(list)\n",
    "good_neurons = set(good_neurons)\n",
    "for component_number in range(1, segmentation.max() + 1):\n",
    "    if component_number not in good_neurons:\n",
    "        continue\n",
    "    current_component_area = segmentation == component_number\n",
    "    value_red = np.percentile(image.get_channel(0)[current_component_area], 75)\n",
    "    # radius is 3, 9, 9 because voxel size is 210x70x70nm and voxel size is in pixels, not physicla units\n",
    "    dilate_sitk = sitk.BinaryContour(sitk.GetImageFromArray(current_component_area.astype(np.uint8)))\n",
    "    dilate_mask = sitk.GetArrayFromImage(sitk.BinaryDilate(dilate_sitk, (3, 9 ,9)))\n",
    "    # dilate_mask = dilate(current_component_area, (3, 9 ,9), False) # slower than two above lines but combined witrh next gave same result\n",
    "    dilate_mask[current_component_area] = 0\n",
    "    value_green = np.percentile(image.get_channel(1)[current_component_area], 75)\n",
    "    # print(component_number, value_red, value_green)\n",
    "    if value_red > 7000:\n",
    "        if value_green > 7000:\n",
    "            neuron_type_dict[\"granular\"].append(component_number)\n",
    "            continue\n",
    "        else:\n",
    "            neuron_type_dict[\"unexpected\"].append(component_number)\n",
    "            continue\n",
    "    elif value_green > 7000:\n",
    "        neuron_type_dict[\"pyramidal\"].append(component_number)\n",
    "        continue\n",
    "    neuron_type_dict[\"inhibitory\"].append(component_number)\n",
    "print(neuron_type_dict)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save cutted nucleus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join(data_path, \"DMSO_120min_2_1\")\n",
    "for key, value in neuron_type_dict.items():\n",
    "    dir_path = os.path.join(save_path, key)\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "    save_components(image, value, segmentation, dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract nucleus from stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nucleus_dict = defaultdict(list)\n",
    "for neuron_type, compenents_list in neuron_type_dict.items():\n",
    "    for component_number in compenents_list:\n",
    "        im = image.cut_image(segmentation == component_number, replace_mask=True)\n",
    "        nucleus_dict[neuron_type].append((component_number, im))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pack whole segmentation in function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_good_neurons(image: Image, segmentation: np.ndarray):\n",
    "    good_neurons = []\n",
    "    for component_number in range(1, segmentation.max() + 1):\n",
    "        current_component_area = segmentation == component_number\n",
    "        # checking if touch borders \n",
    "        coords = np.nonzero(current_component_area)\n",
    "        if 0 == np.min(coords):\n",
    "            continue\n",
    "        touch = False\n",
    "        for axis_cords, max_size in zip(coords, current_component_area.shape):\n",
    "            if np.max(axis_cords) == max_size - 1:\n",
    "                touch = True\n",
    "                break\n",
    "        if touch:\n",
    "            continue\n",
    "        diameter = Diameter.calculate_property(current_component_area, voxel_size=image.spacing, result_scalar=1)\n",
    "        volume = Volume.calculate_property(current_component_area, voxel_size=image.spacing, result_scalar=1)\n",
    "        # calculate\n",
    "        # print(component_number, (4/3 * pi * (diameter/2)**3)/volume, diameter)\n",
    "        if (4/3 * pi * (diameter/2)**3)/volume > 4.5:\n",
    "            continue \n",
    "        good_neurons.append(component_number)\n",
    "    print(f\"filtered {segmentation.max() - len(good_neurons)}\")\n",
    "    return set(good_neurons)\n",
    "\n",
    "def classify_neurons(image: Image, segmentation: np.ndarray, good_neurons: set):\n",
    "    neuron_type_dict = defaultdict(list)\n",
    "    for component_number in range(1, segmentation.max() + 1):\n",
    "        if component_number not in good_neurons:\n",
    "            continue\n",
    "        current_component_area = segmentation == component_number\n",
    "        value_red = np.percentile(image.get_channel(0)[current_component_area], 75)\n",
    "        # radius is 3, 9, 9 because voxel size is 210x70x70nm and voxel size is in pixels, not physicla units\n",
    "        dilate_sitk = sitk.BinaryContour(sitk.GetImageFromArray(current_component_area.astype(np.uint8)))\n",
    "        dilate_mask = sitk.GetArrayFromImage(sitk.BinaryDilate(dilate_sitk, (3, 9 ,9)))\n",
    "        # dilate_mask = dilate(current_component_area, (3, 9 ,9), False) # slower than two above lines but combined witrh next gave same result\n",
    "        dilate_mask[current_component_area] = 0\n",
    "        value_green = np.percentile(image.get_channel(1)[current_component_area], 75)\n",
    "        # print(component_number, value_red, value_green, np.percentile(image.get_channel(1)[current_component_area], 50))\n",
    "        if value_red > 7000:\n",
    "            if value_green > 7000:\n",
    "                neuron_type_dict[\"granular\"].append(component_number)\n",
    "                continue\n",
    "            else:\n",
    "                neuron_type_dict[\"unexpected\"].append(component_number)\n",
    "                continue\n",
    "        elif value_green > 7000:\n",
    "            neuron_type_dict[\"pyramidal\"].append(component_number)\n",
    "            continue\n",
    "        neuron_type_dict[\"inhibitory\"].append(component_number)\n",
    "    if \"unexpected\" in neuron_type_dict:\n",
    "        items = len(neuron_type_dict['unexpected'])\n",
    "        print(f\"deleted {items} neurons {neuron_type_dict['unexpected']}\")\n",
    "        del neuron_type_dict[\"unexpected\"]\n",
    "    # print(neuron_type_dict)\n",
    "    return neuron_type_dict\n",
    "    \n",
    "def segmentation_function(path_to_file, segment_object):\n",
    "    def empty(_x, _y):\n",
    "        pass\n",
    "    image = ImageReader.read_image(path_to_file)\n",
    "    segment_object.set_image(image)\n",
    "    result = segment_object.calculation_run(empty)\n",
    "    segmentation = result.segmentation\n",
    "    \n",
    "    good_neurons = get_good_neurons(image, segmentation)\n",
    "    segmentation = convex_fill(segmentation)\n",
    "    classified_neurons = classify_neurons(image, segmentation, good_neurons)\n",
    "    \n",
    "    nucleus_dict = defaultdict(list)\n",
    "    for neuron_type, compenents_list in classified_neurons.items():\n",
    "        for component_number in compenents_list:\n",
    "            im = image.cut_image(segmentation == component_number, replace_mask=True)\n",
    "            nucleus_dict[neuron_type].append((component_number, im))\n",
    "    return nucleus_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_path, \"segment_data.json\"), 'r') as ff:\n",
    "    segmentation_description = json.load(ff, object_hook=StackSettings.decode_hook)\n",
    "    parameters = segmentation_description[\"values\"]\n",
    "# this change is done to better filter components which contains few nucleus\n",
    "# then I calculate convex hull inside segmentation_function\n",
    "parameters[\"use_convex\"] = False\n",
    "Algorithm = mask_algorithm_dict[segmentation_description[\"algorithm\"]]\n",
    "segment = Algorithm()\n",
    "segment.set_parameters(**parameters)\n",
    "\n",
    "cutted_neurons_dict = dict()\n",
    "\n",
    "for file_path in glob(os.path.join(data_path, \"*.lsm\")):\n",
    "    print(\"proces\", os.path.basename(file_path))\n",
    "    cutted_neurons_dict[file_path] = segmentation_function(file_path, segment)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup inside neuron segmentation algorithm\n",
    "It can be setted manully like in stack segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_path, \"neuron_types_segmentation.json\"), 'r') as ff:\n",
    "    neuron_segmentation_description = json.load(ff, object_hook=PartSettings.decode_hook)\n",
    "\n",
    "neuron_segmentation_profile = neuron_segmentation_description[\"neuron_types\"]\n",
    "NeuronAlgoritm = analysis_algorithm_dict[neuron_segmentation_profile.algorithm]\n",
    "neuron_segment = NeuronAlgoritm()\n",
    "neuron_segment.set_parameters(**neuron_segmentation_profile.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup measurment\n",
    "It can be setted manully using classes from `utils.analysis.statistic_calculation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_path, \"neuron_types_measurment.json\"), 'r') as ff:\n",
    "    neuron_measurment_description = json.load(ff, object_hook=PartSettings.decode_hook)\n",
    "    \n",
    "print(str(neuron_measurment_description[\"neuron_types\"]))\n",
    "measurment_object = neuron_measurment_description[\"neuron_types\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def empty(_x, _y):\n",
    "    pass\n",
    "measurment_dict = defaultdict(list)\n",
    "for file_path, nucleus_group_dict in cutted_neurons_dict.items():\n",
    "    for nucleus_type, nucleus_list in nucleus_group_dict.items():\n",
    "        for _, nucleus in nucleus_list:\n",
    "            neuron_segment.set_image(nucleus)\n",
    "            neuron_segment.set_mask(nucleus.mask[0])  # do not touch time \n",
    "            neuron_segment.set_parameters(**neuron_segmentation_profile.values)\n",
    "            result = neuron_segment.calculation_run(empty)\n",
    "            measurment_dict[nucleus_type].append(\n",
    "                measurment_object.calculate(nucleus.get_channel(2), result.segmentation, result.full_segmentation,\n",
    "                                            nucleus.mask[0], nucleus.spacing, Units.nm))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measurment_result = []\n",
    "for neuron_type, values in measurment_dict.items():\n",
    "    sub_result = defaultdict(list)\n",
    "    for value_dict in values:\n",
    "        for key, (value, _) in value_dict.items():\n",
    "            sub_result[key].append(value)\n",
    "    measurment_result.append((neuron_type, [(name, np.mean(v), np.std(v)) for name, v in sub_result.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ticks, values = list(zip(*measurment_result))\n",
    "values = list(zip(*values))\n",
    "f, axx = plt.subplots(1, len(values), figsize=(5 * len(values),5))\n",
    "for el, ax in zip(values, axx):\n",
    "    name, mean, std = list(zip(*el))\n",
    "    plt.sca(ax)\n",
    "    plt.title(name[0])\n",
    "    plt.bar(range(len(ticks)), mean)\n",
    "    for i, m, s in zip(range(len(mean)), mean, std):\n",
    "        plt.plot([i, i], [m - s/2, m + s/2], color=\"black\")\n",
    "    plt.xticks(range(len(ticks)), ticks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How preview result in notebook\n",
    "\n",
    "In this part I show show preview result of segmentation in notebook using matplotlib. It can be also done with k3d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt \n",
    "# %matplotlib notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PartSeg.utils.color_image.color_image_base import color_image \n",
    "from PartSeg.utils.color_image.color_image import add_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_app = ImageReader.read_image(os.path.join(data_path, \"DMSO_120min_2_1.lsm\"))\n",
    "segmentation_app, metadata_app = load_stack_segmentation(os.path.join(data_path, \"DMSO_120min_2_1.seg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "layer_num = 28\n",
    "layer = image_app.get_layer(0, layer_num)\n",
    "components_to_show = np.ones(segmentation_app.max()+1, dtype=np.uint8)\n",
    "colored_image = color_image(layer, colors=[\"BlackRed\", \"BlackGreen\", \"BlackBlue\", None], min_max=image_app.get_ranges())\n",
    "add_labels(colored_image, segmentation_app[layer_num], 1, True, 2, components_to_show)\n",
    "plt.figure(figsize=(9, 9))\n",
    "plt.imshow(colored_image)\n",
    "plt.show()\n",
    "# plt.imshow(result.segmentation[layer_num])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optional with interactive layer change "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tifffile\n",
    "colored_stack = []\n",
    "for i in range(image_app.layers):\n",
    "    layer = image_app.get_layer(0, i)\n",
    "    colored_image = color_image(layer, colors=[\"BlackRed\", \"BlackGreen\", \"BlackBlue\", None], min_max=image_app.get_ranges())\n",
    "    add_labels(colored_image, segmentation_app[i], 1, True, 2, components_to_show)\n",
    "    colored_stack.append(colored_image)\n",
    "colored_array = np.stack(colored_stack)\n",
    "tifffile.imshow(colored_array)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save stack segmentation\n",
    "\n",
    "how to save segmentation result to easy preview in PartSeg mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from glob import glob \n",
    "\n",
    "from PartSeg.utils.mask.io_functions import SaveSegmentation, SegmentationTuple\n",
    "from PartSeg.utils.segmentation.segmentation_algorithm import ThresholdAlgorithm\n",
    "from PartSeg.tiff_image.image_reader import ImageReader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_path in glob(os.path.join(data_path, \"*.lsm\")):\n",
    "    print(\"proces\", os.path.basename(file_path))\n",
    "    segment = ThresholdAlgorithm()\n",
    "    segment.set_parameters(**parameters)\n",
    "    image = ImageReader.read_image(file_path)\n",
    "    segment.set_image(image)\n",
    "    result = segment.calculation_run(empty)\n",
    "    segmentation = result.segmentation\n",
    "    seg_tup = SegmentationTuple(image.file_path, image, segmentation, [])\n",
    "    save_path = os.path.splitext(file_path)[0] + \".seg\"\n",
    "    SaveSegmentation.save(save_path, seg_tup, {\"relative_path\": True})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
